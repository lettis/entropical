\documentclass[12pt,a4paper,twoside,english,fleqn,preprint,aps,prb]{revtex4}
\usepackage[T1]{fontenc}
%\usepackage[latin9]{inputenc}
\usepackage[utf8]{inputenc}
\setlength{\parskip}{\medskipamount}
\setlength{\parindent}{0pt}
\usepackage{tikz}
\usetikzlibrary{arrows,shapes,positioning,decorations.pathreplacing}
\usepackage{standalone}
\usepackage{rotating}
\usepackage{units}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amstext}
\usepackage{mathrsfs}

\usepackage{graphicx}
\usepackage{caption}
%\usepackage{wrapfig}

\usepackage{amssymb}
\usepackage{esint}
\usepackage{babel}
\usepackage{array}

% needed for placing figures the right way %
\usepackage{float}
\usepackage{color}

\newcommand{\florian}[1] { \emph{ {\color{red}Florian: #1} } }
\newcommand{\gerhard}[1] { \emph{ {\color{blue}Gerhard: #1} } }

\newcommand{\dirfig}{imgs}

%\makeatletter

%\textwidth16cm
\textwidth18cm
\textheight23cm
\hoffset-1.5cm
%\voffset-1.5cm
\setlength{\oddsidemargin}{0.3cm}
\setlength{\evensidemargin}{0.3cm}
%\setlength{\oddsidemargin}{1.6cm}
%\setlength{\evensidemargin}{0.6cm}
%\renewcommand{\arraystretch}{0.5}

%\makeatother

\newcommand{\bm}{\boldmath}
\newcommand{\halb}{\frac{1}{2}}
\newcommand{\be}{\begin{equation*}}
\newcommand{\ee}{\end{equation*}}
\newcommand{\Tr}[2]{{\rm Tr}_{\rm #1}\left\{ #2 \right\}}


\newcommand{\F}{\mathbf{F}}
\newcommand{\rr}{\mathbf{r}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\pp}{\mathbf{p}}
\newcommand{\xx}{\mathbf{x}}
\newcommand{\qq}{\mathbf{q}}
\newcommand{\MU}{\boldsymbol{\mu}}
\newcommand{\RR}{\mathbf{R}}
\newcommand{\oft}{\left(t\right)}
%\newcommand{\unit}[1]{\ensuremath{\, \mathrm{#1}}}
\newcommand{\atantwo}{\text{atan2}}

\newcommand{\T}[1]{\text{#1}}



% macro definitions
\global\long\def\inftyint{\int_{-\infty}^{\infty}}
\global\long\def\bra#1{\left\langle #1\right|}
\global\long\def\ket#1{\left|#1\right\rangle }
\global\long\def\tr{\text{tr}}
% end macro definitions

% define codelisting environment
\newenvironment{codelisting}
{\begin{list}{}{\setlength{\leftmargin}{1em}}\item\bfseries}
{\end{list}}
% end definition


%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{document}

\author{Florian Sittel, Gerhard Stock} 
\affiliation{Biomolecular Dynamics, Institute of Physics, Albert Ludwigs
 University, 79104 Freiburg, Germany}
\title{Analyzing Principal Component Interdependence\\ using Transfer Entropy Estimation}
\date{\today}

% no page break after title
\begingroup
 \let\clearpage\relax
 \maketitle
\endgroup

\tableofcontents
\clearpage


\section{Introduction}
Principal component analysis (PCA), widely adopted among the MD community to drastically reduce the dimensionality
of simulation data of biomolecules, rotates the coordinate system of a given trajectory along its principal
axes, i.e., the axes of maximum variance.
The resulting coordinates (principal components, PCs) are linearly decoupled and ordered from highest to lowest variance.
The reduction in dimensionality is done by selecting a limited number of PCs that show significant dynamics.
Ideally, these PCs describe independent dynamical modes, account for most of the system's dynamics.
However, since molecular dynamics force fields are usually non-linear, this is only an approximation.
In general, PCs influence each other and the dynamics are still (non-linearly) coupled.
Additionally, it is not a priori clear how one can distinguish PCs with significant vs. PCs with non-significant dynamics.
A common way to select PCs is by projecting the free energy on a one-dimensional histogram along the PC in question and checking
for non-random (i.e. non-Gaussian) dynamics.
This, however, may lead to erroneously dismissing of PCs that show Gaussian dynamics on a time average, but may yield
non-random behavior when analyzed along the time series.
A good example for such a behavior is a collective variable that fluctuates with different variances around the same mean,
depending on the current system state.

In order to select PCs for further analysis, without the risk of dismissing essential modes, we propose a
more effective measure by adopting the notion of transfer entropies, as described by Schreiber.\cite{Schreiber2000}
The transfer entropy $T_{J \rightarrow I}$ describes the information flow from a random variable $J$ to a (possibly coupled) random variable $I$.
In our case, we identify the random variables as the different PCs of our trajectory, providing a measure for the coupling of two different collective coordinates.
The transfer entripy is closely related to a cumulative fluctuation,
but takes the dynamic behavior of both collective coordinates into account, too.
Therefore, transfer entropies are in general not symmetric, i.e., $T_{J \rightarrow I} \not= T_{I \rightarrow J}$, since over time
more information may flow from one collective variable to the other.


\section{Theory}
\subsection{Transfer Entropies}
Based on a finite sample of $N$ frames of the system variables $X$ and $Y$,
Schreiber\cite{Schreiber2000} derives the transfer entropy from collective variable $Y$ to $X$ as
\begin{align}
  T_{Y \rightarrow X} &= \sum_{n=1}^{N-1} p(x_{n+1}, x_n, y_n) \log \left[\frac{p(x_{n+1} \vert x_n, y_n) }{ p(x_{n+1} \vert x_n)} \right]\label{eq:T_Schreiber}.
\end{align}
Essentially, the elements of the sum express the influence of $Y$ on the respective next frame $x_{n+1}$ of $X$, normalized by the history of $X$ itself.

Regarding the respective next frame of the trajectory may be a natural choice, however, depending on the regarded system,
collective variables may yield a lagged influence on each other that manifests at longer timescales.
Therefore, Hlaváčková-Schindler et al.\cite{Schindler2007} generalized the original notion of transfer entropies to
\begin{align}
    T_{Y \rightarrow X}^{\tau} &=
        \sum_{n=1}^{N-\tau} p(x_{n+\tau}, x_n, y_n) \log \left[\frac{p(x_{n+\tau} \vert x_n, y_n) }{ p(x_{n+\tau} \vert x_n)} \right]\label{eq:T_Schindler},
\end{align}
with arbitrary lag times $\tau$.

Given a set of $N$ frames, however, the probabilities conditional on $x_n$ and $y_n$ are not known.
Yet, employing Bayes' formalism, we can rewrite Eq. (\ref{eq:T_Schindler}) in the form
\begin{align}
  T_{Y \rightarrow X}^{\tau} &= \sum_{n=1}^{N-\tau} p(x_{n+\tau}, x_n, y_n) \log \left[\frac{p(x_{n+\tau} \vert x_n, y_n) }{ p(x_{n+\tau} \vert x_n)} \right]\nonumber\\
                             &= \sum_{n=1}^{N-\tau} p(x_{n+\tau}, x_n, y_n) \log \left[\frac{p(x_{n+\tau}, x_n, y_n)}{p(x_n, y_n) p(x_{n+\tau} \vert x_n)} \right]\nonumber\\
                             &= \sum_{n=1}^{N-\tau} p(x_{n+\tau}, x_n, y_n) \log \left[\frac{p(x_{n+\tau}, x_n, y_n) p(x_n)}{p(x_n, y_n) p(x_{n+\tau}, x_n)} \right].\label{eq:T}
\end{align}
This way, the transfer entropies are based on joint probabilities only, which can be estimated from the underlying data set.


\subsection{Kernel Density Estimation}
For a robust estimation of the joint probabilities, we use the formalism of kernel density estimation.
By choosing a suitable kernel, e.g., the Gaussian kernel
\begin{align}
  K(u) &= \frac{1}{\sqrt{2\pi}} \exp(-\frac{1}{2} u^2),
\end{align}
we construct a smooth representation of the continuous probability density from finite data.
An estimate of the local probability $\hat{p}$ at point $\xx$ in $d$ dimensions is then given by the product kernel sum over all $N$ frames
\begin{align}
  \hat{p}(\xx) &= \frac{1}{N h_1 \dots h_d} \sum_{i=1}^{N}\left[ \prod_{j=1}^{d} K\left(\frac{x_j - X_{ij}}{h_j} \right)\right].
\end{align}
Here, $X_{ij}$ are the coordinates of frame $i$ in dimension $j$, while $h_j$ is the preselected bandwidth of the kernel for dimension $j$.

However, the choice of the optimal bandwidth $h$ is in general not clear.
Worse, a badly chosen bandwidth has a higher negative influence on the resulting probability density estimation
than an inappropriately chosen kernel function.
Still, using multivariate Gaussian product kernels, we can estimate the bandwidth for each dimension
using Scott's rule of thumb
\begin{align}
  \hat{h}_j = N^{-1/(d+4)} \hat{\sigma}_j,
\end{align}
from the underlying data of $N$ frames in $d$ dimensions and with standard deviations $\sigma_j$ in the respective dimension,
which usually gives satisfying results.

The joint probabilities in (\ref{eq:T}) are thus properly estimated by
\begin{align}
  \hat{p}(x_{n+\tau}, x_n, y_n) &= \frac{N^{-4/7}}{(2\pi)^{3/2} \hat{\sigma}_x^2 \hat{\sigma}_y} \underbrace{\sum_{i=1}^N \exp \left(-\frac{1}{2} \left[
                                       \frac{(x_{n+\tau}-x_i)^2 + (x_n - x_i)^2}{N^{-2/7}\hat{\sigma}_x^2} + \frac{(y_n - y_i)^2}{N^{-2/7}\hat{\sigma}_y^2}
                                 \right] \right)}_{\Sigma_{xxy}}  \\
                                 \hat{p}(x_n, y_n) &= \frac{N^{-5/7}}{2\pi \hat{\sigma}_x \hat{\sigma}_y} \underbrace{\sum_{i=1}^N \exp \left(-\frac{1}{2} \left[
                                       \frac{(x_n - x_i)^2}{N^{-2/7}\hat{\sigma}_x^2} + \frac{(y_n - y_i)^2}{N^{-2/7}\hat{\sigma}_y^2}
                                 \right] \right)}_{\Sigma_{xy}}  \\
                                 \hat{p}(x_{n+\tau}, x_n) &= \frac{N^{-5/7}}{2\pi \hat{\sigma}_x^2} \underbrace{\sum_{i=1}^N \exp \left(-\frac{1}{2} \left[
                                       \frac{(x_{n+\tau} - x_i)^2 + (x_n - x_i)^2}{N^{-2/7}\hat{\sigma}_x^2}
                                 \right] \right) }_{\Sigma_{xx}} \\
                                   \hat{p}(x_n) &= \frac{N^{-6/7}}{(2\pi)^{-1/2} \hat{\sigma}_x} \underbrace{\sum_{i=1}^N \exp \left(-\frac{1}{2} \left[
                                       \frac{(x_n - x_i)^2}{N^{-2/7}\hat{\sigma}_x^2}
                                 \right] \right)}_{\Sigma_{x}}.
\end{align}

Using these definitions, the estimated transfer entropy from $Y$ to $X$ is given by
\begin{align}
  \hat{T}_{Y \rightarrow X}^{\tau} &= \frac{N^{-4/7}}{(2\pi)^{3/2}\sigma_x^2 \sigma_y}
                                        \sum_{n=1}^{N-\tau} \Sigma_{xxy} \log\left(2\pi \frac{\Sigma_{xxy} \Sigma_{x}}{\Sigma_{xy} \Sigma_{xx}} \right).
\end{align}


\bibliography{stock,md,new}

\end{document}
