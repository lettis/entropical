#!/usr/bin/env python3

import argparse
import numpy as np
import pyopencl as cl
import pyopencl.array as cl_array
import pandas as pd
from datetime import datetime
import sys

def to_array(fname, column):
  """load column from file as numpy array"""
  return np.array(
            pd.read_table(fname
                        , sep=" "
                        , engine='c'
                        , skipinitialspace=True
                        , dtype=np.float32
                        , usecols=(column,)
                        , header=None)).flatten()

# command line parsing
parser = argparse.ArgumentParser("transs_ocl")
parser.add_argument("-i", "--input", dest="input_file", required=True,
                      help="input data; plain text, observables in columns, observations in rows.")
parser.add_argument("--pcmax", dest="pcmax", type=int, default=2,
                      help="max PC (i.e. observable) to take into account")
parser.add_argument("-t", "--tau", dest="tau", type=int, default=1,
                      help="tau value (lagtime; default: 1)")
parser.add_argument("--wgsize", dest="wgsize", type=int, default=128,
                      help="number of work items in workgroup. tweak this to get optimal performance from OpenCL. (default: 128)")
args = parser.parse_args()
## setup OpenCL
mf = cl.mem_flags
# one context for all GPUs
devices = cl.get_platforms()[0].get_devices()
n_devices = len(devices)
ctx = cl.Context(devices)
queues = [cl.CommandQueue(ctx, device=d) for d in devices]
# determine number of rows in input file
n_rows = to_array(args.input_file, 0).size
# kernel: partial_sums
#   build options to set:
#     TAU:    the tau value (some unsigned integer > 0)
#     N_ROWS: number of rows in input (== to size of x and y)
#     WGSIZE: local size, i.e. number of worker items in workgroup
src_partial_sums = """
  #define POW2(X) (X)*(X)
  #define TWO_PI 6.283185307179586

  __kernel void
  partial_sums(uint n
             , float p_s_x
             , float p_s_y
             , __global const float* y
             , __global const float* x
             , uint iy
             , uint ix
             , __global float4* S
             , __global float* T) {
    __local float4 s_tmp[WGSIZE];
    int gid = get_global_id(0);
    if (gid < N_ROWS) {
      int lid = get_local_id(0);
      int wid = get_group_id(0);
      // retrieve data from global memory
      float x_n = x[n];
      float x_ntau = x[n+TAU];
      float x_i = x[gid];
      float y_n = y[n];
      float y_i = y[gid];
      // compute local values
      float tmp_yn = exp(p_s_y * POW2(y_n-y_i));
      float s_x = exp(p_s_x * POW2(x_n-x_i));
      float s_xxy = exp(p_s_x * POW2(x_ntau-x_i)) * s_x * tmp_yn;
      float s_xy = s_x * tmp_yn;
      float s_xx = POW2(s_x);
      s_tmp[lid] = (float4) (s_x, s_xxy, s_xy, s_xx);
      barrier(CLK_LOCAL_MEM_FENCE);
      // accumulate s-values locally
      if (lid == 0) {
        for (uint i=1; i < WGSIZE; ++i) {
          s_tmp[0] += s_tmp[i];
        }
        S[lid] = s_tmp[0];
      }
      barrier(CLK_GLOBAL_MEM_FENCE);
      // accumulate s-values globally
      if (wid == 0) {
        float4 S_acc = S[0];
        uint n_wg = get_num_groups(0);
        for (uint i=1; i < n_wg; ++i) {
          S_acc += S[i];
        }
        T[iy*PCMAX+ix] += S_acc.s1 * log(TWO_PI * S_acc.s1 * S_acc.s0 / S_acc.s2 / S_acc.s3);
      }
    }
  }
"""
# compile kernel
prg = cl.Program(ctx, src_partial_sums).build(options=["-D", "TAU=%d" % args.tau
                                                     , "-D", "N_ROWS=%d" % n_rows
                                                     , "-D", "WGSIZE=%d" % args.wgsize
                                                     , "-D", "PCMAX=%d" % args.pcmax])
knl_partial_sums = prg.partial_sums
# memory handling
n_workgroups = np.ceil(n_rows / args.wgsize)
global_size = n_workgroups * args.wgsize
T = np.zeros(args.pcmax**2, dtype=np.float32)
T_correction_factors = np.zeros(args.pcmax**2)
T_final = np.zeros(args.pcmax**2, dtype=np.float32)
T_buf = cl.Buffer(ctx, mf.READ_WRITE | mf.COPY_HOST_PTR, hostbuf=T)
knl_partial_sums.set_arg(8, T_buf)
S = np.zeros(n_workgroups, dtype=cl_array.vec.float4)
S_buf = cl.Buffer(ctx, mf.READ_WRITE | mf.COPY_HOST_PTR, hostbuf=S)
knl_partial_sums.set_arg(7, S_buf)
y = np.zeros(n_rows, dtype=np.float32)
y_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=y)
knl_partial_sums.set_arg(3, y_buf)
x = np.zeros(n_rows, dtype=np.float32)
x_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=x)
knl_partial_sums.set_arg(4, x_buf)
# compute transfer entropies
for iy in range(args.pcmax):
  knl_partial_sums.set_arg(5, np.uint32(iy))
  y = to_array(args.input_file, iy)
  for i_dev in range(n_devices):
    cl.enqueue_copy(queues[i_dev], y_buf, y)
  sigma_y = np.sqrt(np.var(y))
  p_s_y = -1.0 / (2*np.power(y.size, -2.0/7.0)*sigma_y**2);
  knl_partial_sums.set_arg(2, np.float32(p_s_y))
  # separate ix-range into groups of IDs for multiple GPUs
  ix_ranges = [a.tolist() for a in np.array_split(np.arange(args.pcmax), n_devices)]
  while len(ix_ranges[0]) > 0:
    # distribute work to GPUs
    for i_dev in range(n_devices):
      if len(ix_ranges[i_dev]) > 0:
        cl.enqueue_barrier(queues[i_dev])
        ix = ix_ranges[i_dev].pop()
        print("run ", iy, ix, datetime.now(), file=sys.stderr, flush=True)
        knl_partial_sums.set_arg(6, np.uint32(ix))
        x = to_array(args.input_file, ix)
        cl.enqueue_copy(queues[i_dev], x_buf, x)
        sigma_x = np.sqrt(np.var(x))
        p_s_x = -1.0 / (2*np.power(x.size, -2.0/7.0)*sigma_x**2);
        knl_partial_sums.set_arg(1, np.float32(p_s_x))
        for n in range(n_rows):
          knl_partial_sums.set_arg(0, np.uint32(n))
          cl.enqueue_nd_range_kernel(queues[i_dev], knl_partial_sums, (int(global_size),), (args.wgsize,))
        T_correction_factors[iy*args.pcmax+ix] = np.power(n_rows, -4.0/7.0) / (np.power(2*np.pi, 3.0/2.0) * sigma_x**2 * sigma_y)
# retrieve results
for i_dev in range(n_devices):
  cl.enqueue_barrier(queues[i_dev])
  cl.enqueue_copy(queues[i_dev], T, T_buf)
  T_final += T
for iy in range(args.pcmax):
  for ix in range(args.pcmax):
    print(" ", T_final[iy*args.pcmax+ix] * T_correction_factors[iy*args.pcmax+ix], end="")
  print("")
print("finished ", datetime.now(), file=sys.stderr)

